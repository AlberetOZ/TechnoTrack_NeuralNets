{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# создаем энвайронмент с игрой\n",
    "env = gym.make('Breakout-v0').unwrapped\n",
    "\n",
    "# настраиваем matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display    \n",
    "print(\"Is python : {}\".format(is_ipython))\n",
    "\n",
    "# выбираем девайс для игры\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device : {}\".format(device))\n",
    "\n",
    "# запоминаем, сколько действий в игре\n",
    "ACTIONS_NUM = env.action_space.n\n",
    "print(\"Number of actions : {}\".format(ACTIONS_NUM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 4\n",
    "STATE_W = 84\n",
    "STATE_H = 84\n",
    "MEMSIZE = 55000\n",
    "\n",
    "# Задание 1. Необходимо реализовать класс для хранения состояния игры. \n",
    "# В качестве последнего мы будем использовать состеканные 4 последовательных кадра игры.\n",
    "# Это необходимо, чтобы агент понимал скорости и ускорения игровых объектов.\n",
    "class StateHolder:\n",
    "    state_size = 4\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.screens = []\n",
    "        \n",
    "        for i in range(self.state_size):\n",
    "            env.step(0)\n",
    "            self.screens.append(get_screen())\n",
    "       \n",
    "    def push(self, screen):\n",
    "        self.screens.pop(0)\n",
    "        self.screens.append(screen)\n",
    "            \n",
    "    def get(self):\n",
    "        return torch.stack(self.screens).squeeze(1)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.screens = []\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity = MEMSIZE):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        #Положить переход в память\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 2. Собрать архитектуру сети (DQN).\n",
    "# В качестве примера можно использовать сеть вида:\n",
    "# Conv(4->32) -> Conv(32->64) -> Conv(64->64) -> FC(512) -> FC(ACTIONS_NUM)\n",
    "# В качестве функций активации необходимо использовать ReLU(но совершенно не обязательно ими ограничиваться)\n",
    "# Attention : не забудьте правильно инициализировать веса, это важно для данной задачи!\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=4):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 32, kernel_size=8, stride=4), nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU())\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU())\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут блок с кодом, генерирующим 1 кадр игры\n",
    "# Обратите внимание, что выходным тензора является torch.ByteTensor со значениями 0-255\n",
    "# Это сделанно намеренно для экономии места(4х экономия по сравнению с FloatTensor)\n",
    "# Подумайте, где и как необходимо совершать преобразование ByteTensort -> FloatTensor, чтобы его можно было подавать в сеть. \n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize( (STATE_W, STATE_H), interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    screen = np.dot(screen[...,:3], [0.299, 0.587, 0.114])\n",
    "    screen = screen[30:195,:]\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.uint8).reshape(screen.shape[0],screen.shape[1],1)\n",
    "    return resize(screen).mul(255).type(torch.ByteTensor).to(device).detach()\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().reshape(-1,84).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Далее стандартный метод для выбора нового действия из лекции\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=0.0002,  alpha=0.95, eps=0.01)\n",
    "\n",
    "memory = ReplayMemory()\n",
    "\n",
    "def select_action(state, eps_threshold):\n",
    "    sample = random.random()\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state.unsqueeze(0).float()).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(ACTIONS_NUM)]], device=device, dtype=torch.long)\n",
    "\n",
    "train_rewards = []\n",
    "\n",
    "mean_size = 100\n",
    "mean_step = 1\n",
    "\n",
    "def plot_rewards(rewards = train_rewards, name = \"Train\"):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(rewards)\n",
    "    # Строим график среднего вознаграждения по 100 последним эпизодам\n",
    "    if len(rewards) > mean_size:\n",
    "        means = np.array([rewards[i:i+mean_size:] for i in range(0, len(rewards) - mean_size, mean_step)]).mean(1)\n",
    "        means = np.concatenate((np.zeros(mean_size - 1), means))\n",
    "        plt.plot(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Еще немного методов из лекции\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    # выбираем новый батч\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Для всех состояний считаем маску не финальнсти и конкантенируем их\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]).reshape(-1, 4, 84, 84).float()\n",
    "    \n",
    "    state_batch = torch.cat(batch.state).reshape(32, 4, 84, 84).float()\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Считаем Q(s_t, a) - модель дает Q(s_t), затем мы выбираем\n",
    "    # колонки, которые соответствуют нашим действиям на щаге\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Подсчитываем ценность состяония V(s_{t+1}) для всех последующмх состояний.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device).detach()\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach() # берем значение максимума\n",
    "    \n",
    "    # Считаем ожидаемое значение функции оценки ценности действия  Q-values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Считаем ошибку Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Оптимизация модели\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    del non_final_mask\n",
    "    del non_final_next_states\n",
    "    del state_batch\n",
    "    del action_batch\n",
    "    del reward_batch\n",
    "    del state_action_values\n",
    "    del next_state_values\n",
    "    del expected_state_action_values\n",
    "    del loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# количество эпизодов, которые играем\n",
    "NUM_EPISODES = 5000\n",
    "\n",
    "# количество кадров, между которыми обучаем модель\n",
    "OPTIMIZE_MODEL_STEP = 5\n",
    "# количество кадров, между которыми обновляем target-модель\n",
    "TARGET_UPDATE= 10000\n",
    "\n",
    "# несколько шагов для разогрева модели()\n",
    "STEPS_BEFORE_TRAIN = 55000\n",
    "# параметры для e-greedy стратегии выбора действия\n",
    "EPS_START = 1\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 1000000\n",
    "\n",
    "policy_net.train()\n",
    "target_net.eval()\n",
    "\n",
    "test_rewards = []\n",
    "\n",
    "# Общее число \n",
    "steps_done = 0\n",
    "eps_threshold = EPS_START\n",
    "\n",
    "# Задание 3. Заполнить пропуски в нижеприведенном коде.\n",
    "\n",
    "for e in tqdm.tqdm(range(NUM_EPISODES)):\n",
    "    \n",
    "    # Инициализации разлинычх переменных\n",
    "    # env - среды\n",
    "    env.reset()\n",
    "    # state_holder - хранителя состояния\n",
    "    state_holder = StateHolder(env)\n",
    "\n",
    "    # lives - количества жизней в начале эпизода игры (Важно!)\n",
    "    \n",
    "    ep_rewards = []\n",
    "    cur_lives = 5\n",
    "    \n",
    "    for t in count():\n",
    "        \n",
    "        if eps_threshold < EPS_END:\n",
    "            eps_threshold = EPS_END\n",
    "        else:\n",
    "            eps_threshold -= (EPS_START - EPS_END) / EPS_DECAY\n",
    "        \n",
    "        steps_done+=1        \n",
    "        state = state_holder.get()\n",
    "        \n",
    "        # Шаг одного кадра игры \n",
    "        action = select_action(state, eps_threshold)\n",
    "        _, reward, done, info = env.step(action.item())\n",
    "        state_holder.push(get_screen())\n",
    "        \n",
    "        new_lives = info['ale.lives']\n",
    "        ep_rewards.append(reward)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done or (cur_lives > new_lives):\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = state_holder.get()\n",
    "            \n",
    "        cur_lives = new_lives\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        # Работа с ReplayMemory\n",
    "        \n",
    "        # Задание 3.1. Необходимо дополниь код следующим трюком, который значительно\n",
    "        # улучшает сходимость обучения. В случае, если эпизод не закончился, но агент на \n",
    "        # очередном шаге потерял жизнь, то такой переход надо класть в ReplayMemory как финальный.\n",
    "        # При этом, необходимо далее продолжать эпизод игры, пока не получите done == True\n",
    "        # Тет самым вы научите агента понимать, что терять жизни по дороге - плохо.\n",
    "        \n",
    "        # Шаг оптимизации\n",
    "        \n",
    "        if (steps_done > STEPS_BEFORE_TRAIN) and steps_done % OPTIMIZE_MODEL_STEP == 0:\n",
    "            optimize_model()\n",
    "            \n",
    "        if steps_done == STEPS_BEFORE_TRAIN:\n",
    "            print(\"Start training\")\n",
    "        \n",
    "        # Шаг обновления target'сети\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            print(\"Target net updated!\")\n",
    "            print(\"Eps = \", eps_threshold)\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            torch.save(policy_net.state_dict(), \"model\")\n",
    "        \n",
    "        # Код завершающий эпизод игры/обучения\n",
    "        if done:\n",
    "            train_rewards.append(np.sum(ep_rewards))\n",
    "            # Суммарный reward(не дисконтированный) за последний эпизод\n",
    "            print(\"Episode score : {}\".format(train_rewards[-1]))\n",
    "            # Средний reward по последним 100 эпизодам\n",
    "            print(\"Mean score : {}\".format(np.mean(train_rewards[-100:])))\n",
    "            \n",
    "            plot_rewards()\n",
    "            break \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут код для тестирования.\n",
    "# Задание 4. Просто выполнить данную ячейку и проверить вашего агента, насколько он хорош !?\n",
    "#policy_net.load_state_dict(torch.load(\"model\"))\n",
    "eps_threshold = 0.05\n",
    "\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (env.spec.id, step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "policy_net.eval()\n",
    "env.reset()\n",
    "state_holder = StateHolder(env)\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for i in count():\n",
    "    # Выбрать и выполнить нове действие\n",
    "    action = select_action(state_holder.get(), eps_threshold)\n",
    "    _, reward, done, _ = env.step(action.item())\n",
    "    total_reward += reward\n",
    "    # Получаем новое состояние\n",
    "    if not done:\n",
    "        state_holder.push(get_screen())\n",
    "    else:\n",
    "        break\n",
    "    show_state(env, i)\n",
    "    \n",
    "print(\"Total game reward : {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
