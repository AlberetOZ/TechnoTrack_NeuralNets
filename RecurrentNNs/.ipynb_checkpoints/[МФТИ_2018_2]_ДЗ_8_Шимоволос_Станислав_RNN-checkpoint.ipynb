{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'ololoasdasddqweqw123456789'\n",
    "# word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        #print(self.chars2idx)\n",
    "        #print(self.indexs)\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        \n",
    "        self.n_a = hidden_size\n",
    "        self.n_x = in_size\n",
    "        self.n_y = out_size\n",
    "        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, lr=0.1, n_epochs=100, CLIP_GRAD=True, max_norm=5):\n",
    "    optim  = SGD(net.parameters(), lr, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        hh = torch.zeros(net.n_a)\n",
    "        loss = 0\n",
    "        optim.zero_grad()\n",
    "        for sample, next_sample in ds:\n",
    "            x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "            target =  torch.LongTensor([next_sample])\n",
    "            y, hh = net(x, hh)\n",
    "            loss += criterion(y, target)\n",
    "            \n",
    "        loss.backward()\n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "            print (loss.data.item())\n",
    "            if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm))\n",
    "        else: \n",
    "            if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm)\n",
    "    \n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.42355346679688\n",
      "Clip gradient :  6.600681159038764\n",
      "65.40913391113281\n",
      "Clip gradient :  4.805861910842513\n",
      "54.41361618041992\n",
      "Clip gradient :  5.328119321187901\n",
      "41.039649963378906\n",
      "Clip gradient :  4.976227012313821\n",
      "30.203020095825195\n",
      "Clip gradient :  2.8555914418112227\n",
      "24.489974975585938\n",
      "Clip gradient :  2.448440428206962\n",
      "21.300498962402344\n",
      "Clip gradient :  1.568093778378552\n",
      "19.28201675415039\n",
      "Clip gradient :  1.7587875163415618\n",
      "17.760635375976562\n",
      "Clip gradient :  1.289146025764035\n",
      "17.5850772857666\n",
      "Clip gradient :  38.295315676671294\n",
      "15.97107219696045\n",
      "Clip gradient :  12.313857602049628\n",
      "16.357086181640625\n",
      "Clip gradient :  45.789410182594644\n",
      "15.648397445678711\n",
      "Clip gradient :  28.645256490782295\n",
      "14.555351257324219\n",
      "Clip gradient :  9.488562717721312\n",
      "13.638294219970703\n",
      "Clip gradient :  7.97922947124924\n"
     ]
    }
   ],
   "source": [
    "train(rnn, lr=0.01, n_epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololololololololololololol\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-612b341693d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Prediction:\\t'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpredword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_cell(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super(LSTM_cell, self).__init__()\n",
    "    \n",
    "        self.n_a = hidden_size\n",
    "        self.n_x = in_size\n",
    "        self.n_y = out_size\n",
    "        \n",
    "        self.forget_gate = nn.Sequential(nn.Linear(in_size + hidden_size, hidden_size), nn.Sigmoid())\n",
    "        self.update_gate = nn.Sequential(nn.Linear(in_size + hidden_size, hidden_size), nn.Sigmoid())\n",
    "        self.output_gate = nn.Sequential(nn.Linear(in_size + hidden_size, hidden_size), nn.Sigmoid())\n",
    "        self.cand_cell  = nn.Sequential(nn.Linear(in_size + hidden_size, hidden_size), nn.Tanh())\n",
    "        self.out_weight = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "        self.hidden_activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, prev_hidden):\n",
    "        concat = torch.cat((prev_hidden.squeeze(0), x.squeeze(0)))\n",
    "        \n",
    "        ft = self.forget_gate(concat)\n",
    "        it = self.update_gate(concat)\n",
    "        ot = self.output_gate(concat)\n",
    "        \n",
    "        cct = self.cand_cell(concat)\n",
    "        c_next = cct * it + prev_hidden * ft\n",
    "        a_next = ot * self.hidden_activation(c_next)\n",
    "        \n",
    "        output = self.out_weight(a_next).unsqueeze(0)\n",
    "        return output, a_next\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "lstm_rnn = LSTM_cell(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4686832427978516\n",
      "Clip gradient :  16.469504612403775\n",
      "7.138798713684082\n",
      "Clip gradient :  15.02678278323623\n",
      "4.251806735992432\n",
      "Clip gradient :  2.7824029009647813\n",
      "3.4166812896728516\n",
      "Clip gradient :  2.014304863799851\n",
      "3.014051914215088\n",
      "Clip gradient :  1.137543142335301\n",
      "2.7166848182678223\n",
      "Clip gradient :  0.5987286151986356\n",
      "2.5447959899902344\n",
      "Clip gradient :  0.5014941530755052\n",
      "2.4247255325317383\n",
      "Clip gradient :  0.20293119753805675\n",
      "2.335513114929199\n",
      "Clip gradient :  0.20171146491761935\n",
      "2.2603588104248047\n",
      "Clip gradient :  0.12482335612137305\n",
      "2.2075977325439453\n",
      "Clip gradient :  0.9097749663593979\n",
      "2.2112159729003906\n",
      "Clip gradient :  1.1356064391567149\n",
      "2.1422786712646484\n",
      "Clip gradient :  0.5871379969164257\n",
      "2.079409599304199\n",
      "Clip gradient :  0.35148417782362\n",
      "2.032583236694336\n",
      "Clip gradient :  0.2544282075593267\n",
      "1.9950084686279297\n",
      "Clip gradient :  0.1392638815119641\n",
      "1.962590217590332\n",
      "Clip gradient :  0.08937392892108649\n",
      "1.9338808059692383\n",
      "Clip gradient :  0.07048798747028015\n",
      "1.9079504013061523\n",
      "Clip gradient :  0.07808051742033509\n",
      "7.435681343078613\n",
      "Clip gradient :  39.82820940871602\n",
      "3.664482593536377\n",
      "Clip gradient :  5.250275261599013\n",
      "2.38505220413208\n",
      "Clip gradient :  2.0040839943580484\n",
      "2.1095571517944336\n",
      "Clip gradient :  1.0981671791951777\n",
      "1.9464130401611328\n",
      "Clip gradient :  0.2886379267807249\n",
      "1.8774280548095703\n",
      "Clip gradient :  0.2563368576923154\n",
      "1.8356561660766602\n",
      "Clip gradient :  0.1182016784731785\n",
      "1.8072700500488281\n",
      "Clip gradient :  0.10790176640705036\n",
      "1.7846975326538086\n",
      "Clip gradient :  0.06198058187363371\n",
      "1.7654542922973633\n",
      "Clip gradient :  0.04948533524037167\n",
      "1.7475509643554688\n",
      "Clip gradient :  0.046992050435057504\n"
     ]
    }
   ],
   "source": [
    "train(lstm_rnn, lr=0.1, n_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net):\n",
    "    net = net.eval()\n",
    "    hh = torch.zeros(net.n_a)\n",
    "    id = 0\n",
    "    \n",
    "    softmax  = nn.Softmax(dim=1)\n",
    "    predword = ds.get_char_by_id(id)\n",
    "    for c in enumerate(word[:-1]):\n",
    "        x = ds.get_one_hot(id).unsqueeze(0)\n",
    "        y, hh = net(x, hh)\n",
    "        y = softmax(y)\n",
    "        m, id = torch.max(y, 1)\n",
    "        id = id.data[0]\n",
    "        predword += ds.get_char_by_id(id)\n",
    "    print ('Prediction:\\t' , predword)\n",
    "    print(\"Original:\\t\", word)\n",
    "    assert(predword == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "evaluate(lstm_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_cell(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super(GRU_cell, self).__init__()\n",
    "    \n",
    "        self.n_a = hidden_size\n",
    "        self.n_x = in_size\n",
    "        self.n_y = out_size\n",
    "        \n",
    "        self.update_gate = nn.Sequential(nn.Linear(in_size + hidden_size, hidden_size), nn.Sigmoid())\n",
    "        self.relevance_gate = nn.Sequential(nn.Linear(in_size + hidden_size, hidden_size), nn.Sigmoid())\n",
    "        self.candidate_cell  = nn.Sequential(nn.Linear(in_size + hidden_size, hidden_size), nn.Tanh())\n",
    "        self.out_weight = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "        self.hidden_activation = nn.Tanh()\n",
    "        self.candiate_activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, prev_hidden):\n",
    "        a_prev = prev_hidden.squeeze(0)\n",
    "        concat = torch.cat((a_prev, x.squeeze(0)))\n",
    "        \n",
    "        update_gate = self.update_gate(concat)\n",
    "        relevance_gate = self.relevance_gate(concat)\n",
    "        rel_candidate = relevance_gate * a_prev\n",
    "        \n",
    "        concat = torch.cat((rel_candidate, x.squeeze(0)))\n",
    "        candidate = self.candidate_cell(concat)\n",
    "        \n",
    "        c_next = candidate * (1 - update_gate) + update_gate * a_prev\n",
    "        output = self.out_weight(c_next).unsqueeze(0)\n",
    "        return output, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "gru_rnn = GRU_cell(in_size=ds.vec_size, hidden_size=5, out_size=ds.vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.56558990478516\n",
      "Clip gradient :  3.831838887589271\n",
      "33.33504104614258\n",
      "Clip gradient :  5.853957675093574\n",
      "10.06079387664795\n",
      "Clip gradient :  10.716120767357921\n",
      "5.819554328918457\n",
      "Clip gradient :  5.733841121431512\n",
      "5.50185489654541\n",
      "Clip gradient :  9.764282672122878\n",
      "3.0489377975463867\n",
      "Clip gradient :  2.280550932119522\n",
      "1.0880203247070312\n",
      "Clip gradient :  3.3801880246201286\n",
      "0.3574981689453125\n",
      "Clip gradient :  0.38930667379147355\n",
      "0.44561290740966797\n",
      "Clip gradient :  4.597023382639376\n",
      "0.19213485717773438\n",
      "Clip gradient :  0.5352082028258958\n",
      "0.1529092788696289\n",
      "Clip gradient :  0.5568063524997623\n",
      "0.11562252044677734\n",
      "Clip gradient :  0.13055075322003332\n",
      "0.10169601440429688\n",
      "Clip gradient :  0.05041791309738597\n",
      "0.092987060546875\n",
      "Clip gradient :  0.04977069143156657\n",
      "0.08562374114990234\n",
      "Clip gradient :  0.029451627886872573\n",
      "0.07989692687988281\n",
      "Clip gradient :  0.026834745507326313\n",
      "0.07496356964111328\n",
      "Clip gradient :  0.022148995739883572\n",
      "0.0707244873046875\n",
      "Clip gradient :  0.01960121608615734\n",
      "0.06701946258544922\n",
      "Clip gradient :  0.018272154131423926\n",
      "0.06373310089111328\n",
      "Clip gradient :  0.01720621917755581\n",
      "0.06078338623046875\n",
      "Clip gradient :  0.01632517830666475\n",
      "0.0581207275390625\n",
      "Clip gradient :  0.015571083971130469\n",
      "0.05569267272949219\n",
      "Clip gradient :  0.01489408331464648\n",
      "0.05347633361816406\n",
      "Clip gradient :  0.014282894075525911\n",
      "0.05143165588378906\n",
      "Clip gradient :  0.013723494059182526\n",
      "0.049549102783203125\n",
      "Clip gradient :  0.013211483766367275\n",
      "0.047801971435546875\n",
      "Clip gradient :  0.012738662636440287\n",
      "0.046179771423339844\n",
      "Clip gradient :  0.012301550696721647\n",
      "0.04466724395751953\n",
      "Clip gradient :  0.011895584592538585\n",
      "0.04324913024902344\n",
      "Clip gradient :  0.011515896008189742\n"
     ]
    }
   ],
   "source": [
    "train(gru_rnn, lr=0.1, n_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "evaluate(gru_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
